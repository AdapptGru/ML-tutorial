# Learning Theory

## Capacity
- Neuronal Capacity (Pierre Baldi)
- Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes

## Dimensionality
- On the Dimensionality of Word Embedding (Stanford)
	- PIP loss, bias-variance trade-off

## Convergence
- Phase Retrieval Under a Generative Prior (Paul Hand)
- Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients
- On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport (Francis Bach)
- Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data (Yuanzhi Li)
- Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units

## Optimization
- The Lingering of Gradients: How to Reuse Gradients Over Time (Zeyuan Zhu)
- Submodular:
	- Quadratic Decomposable Submodular Function Minimization (UIUC)
- Natural Gradient
	- Exact natural gradient in deep linear networks and application to the nonlinear case
	- Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis
- Bayesian Distributed Stochastic Gradient Descent (Frank Wood)
- L4: Practical loss-based stepsize adaptation for deep learning
- **How Does Batch Normalization Help Optimization?**

## Loss
- Visualizing the Loss Landscape of Neural Nets

## Analysis
- **Neural Ordinary Differential Equations**
- The Description Length of Deep Learning models (FAIR)

## Solvability	
- Learning to Solve SMT Formulas
