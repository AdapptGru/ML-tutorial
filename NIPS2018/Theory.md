# Learning Theory

## Capacity
- Neuronal Capacity (Pierre Baldi)
- Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes

## Dimensionality
- On the Dimensionality of Word Embedding (Stanford)
	- PIP loss, bias-variance trade-off

## Convergence
- Phase Retrieval Under a Generative Prior (Paul Hand)
- Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients
- On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport (Francis Bach)
- Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data (Yuanzhi Li)
- Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units
- How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective
- On the Local Minima of the Empirical Risk (Berkeley)
- NEON2: Finding Local Minima via First-Order Oracles (Allen Zhu)
- Byzantine Stochastic Gradient Descent (Allen Zhu)
- Are ResNets Provably Better than Linear Predictors?
- Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced (Simon Du, Jason Lee)

## Optimization
- The Lingering of Gradients: How to Reuse Gradients Over Time (Zeyuan Zhu)
- Submodular:
	- Quadratic Decomposable Submodular Function Minimization (UIUC)
	- Optimization for Approximate Submodularity
	- Submodular Maximization via Gradient Ascent: The Case of Deep Submodular Functions
	- Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization (Francis Bach)
	- **Submodular Field Grammars: Representation, Inference, and Application to Image Parsing (Pedro Domingos)**
- Natural Gradient
	- Exact natural gradient in deep linear networks and application to the nonlinear case
	- Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis
- Bayesian Distributed Stochastic Gradient Descent (Frank Wood)
- L4: Practical loss-based stepsize adaptation for deep learning
- Batch-Norm:
	- **How Does Batch Normalization Help Optimization?**
	- Understanding Batch Normalizations
- Stochastic Cubic Regularization for Fast Nonconvex Optimization (M. Jordan)
- Variance Reduction:
	- Stochastic Nested Variance Reduced Gradient Descent for Nonconvex Optimization (UCLA)
	- SEGA: Variance Reduction via Gradient Sketching
- Natasha 2: Faster Non-Convex Optimization Than SGD
- Stochastic Expectation Maximization with Variance Reduction
- Adversarially Robust Optimization with Gaussian Processes
- Learning with SGD and Random Features
- Langevin
	- Mirrored Langevin Dynamics
	- The promises and pitfalls of Stochastic Gradient Langevin Dynamics
- On Markov Chain Gradient Descent (Wotao Yin)

## Loss
- Visualizing the Loss Landscape of Neural Nets

## Analysis
- **Neural Ordinary Differential Equations**
- The Description Length of Deep Learning models (FAIR)
- How Many Samples are Needed to Estimate a Convolutional Neural Network? (Simon Du)

## Solvability	
- Learning to Solve SMT Formulas

## Misc
- Why Is My Classifier Discriminatory?

