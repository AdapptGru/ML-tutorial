# AutoML, Architecture Search

## NASNET, Brain
- 2017: Deeparchitect: Automatically designing and training deep
architectures
- 2018: Neural architecture search with bayesian optimisation and optimal transport.

- RL
 - Zoph, Le: 2016 Neural architecture search with reinforcement learning
 - Zoph, Le (NASNET): 2017 Learning transferable architectures for scalable image recognition.
 - H Pham, M. Guan, B. Zoph, Q. Le (ENAS): 2018 Efficient neural architecture search via parameter sharing https://github.com/melodyguan/enas/tree/2734eb2657847f090e1bc5c51c2b9cbf0be51887
- Evolution:
 - H Liu, K simonyan, et.al.: 2017 Hierarchical representations for efficient architecture search
 - E. Real, Q. Le, (AmoebaNet): 2018 Regularized evolution for image classifier architecture search.
 - AutoKeras 2018: Efficient Neural Architecture Search with Network Morphism
- SMBO
 - 2017 Efficient architecture search by network transformation
 - C. Liu, B. Zoph, J Shlens, W. Hua, A Yuille, K. Murphy (PNAS): 2017 Progressive neural architecture search

## Differential
- Gradient based hyper-parameter opt
 - ICML 2015: Gradient-based hyperparameter optimization through reversible learning
 - ICML 2016: Hyperparameter optimization with approximate gradient.
- DARTS: Differentiable Architecture Search
 - 2.83% error CIFAR, 73.1% ImageNet top-1
 - 56.1 Perplexity PTB
 - https://github.com/quark0/darts
