# Chess and Poker

## Chess
- M. Campbell, A. J. Hoane, and F. hsiung Hsu. Deep blue. Artificial intelligence, 2002.

## Go
- Alpha Go:
	- D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 2014.
	- Mastering the game of Go without human knowledge
- PhoenixGo
	- https://github.com/Tencent/PhoenixGo
- MuGo
	- https://github.com/brilee/MuGo
- Leela
	- https://github.com/gcp/leela-zero
- MiniGo:
	- https://github.com/tensorflow/minigo
- ELF:
	- Y. Tian and Y. Zhu. Better computer go player with neural network and long- term prediction. arxiv, 2015.
	- ELF2
- Legacy:
	- P. Baudis and J. loup Gailly. Pachi: State of the art open source go program. Advances in Computer Games, 2012.
	- C. Clark and A. Storkey. Teaching deep convolutional neural networks to play go. ICML, 2015.
	- M. Enzenberger. The integration of a priori knowledge into a go playing neural network. URL: http://www.markus-enzenberger.de/neurogo.html, 1996.
	- M. Enzenberger. an open-source framework for board games and go engine based on monte carlo tree search. Computational Intelligence and AI in Games, IEEE Transactions on., 2010.
	- C. Maddison, A. Huang, I. Sutskever, and D. Silver. Move evaluation in go using deep convolutional neural networks. arxiv, 2014.
	- D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. ICML, 2014.
	- D. Silver, Temporal-Difference Search in Computer Go, 2012

# Poker (Texas Hod'em)
- Imperfect information Game
- CFR (Counterfactural Regret Minimization)
	- Zinkevich, M., Johanson, M., Bowling, M., & Piccione, C. Regret minimization in games with incomplete information, NIPS 2008
	- Lanctot, M., Waugh, K., Zinkevich, M., & Bowling, M, Monte Carlo sampling for regret minimization in extensive games, NIPS 2009
	- Johanson, M., Bard, N., Lanctot, M., Gibson, R., & Bowling, M, Efficient Nash equilibrium approximation through Monte Carlo counterfactual regret minimization, 2012
	- Johanson, M., Waugh, K., Bowling, M., & Zinkevich, M. Accelerating best response calculation in large extensive games, AAAI 2011
	- Codes: https://github.com/tansey/pycfr
	- A simple (Rock-Paper-Scissors) codes: https://hackernoon.com/artificial-intelligence-poker-and-regret-part-1-36c78d955720
- Noam Brown, Tuomas Sandholm. (CMU)
	- Safe and Nested Subgame Solving for Imperfect-Information Games, NIPS 2017
	- Libratus: The Superhuman AI for No-Limit Poker, IJCAI 2017
	- Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning, ICML 2017